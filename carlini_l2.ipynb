{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, Model, losses, optimizers, metrics\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class MNISTModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))\n",
    "        self.conv2 = Conv2D(32, (3, 3), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), activation='relu')\n",
    "        self.conv4 = Conv2D(128, (3, 3), activation='relu')\n",
    "        self.maxpo = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.batch = BatchNormalization()\n",
    "        self.flatt = Flatten()\n",
    "        self.dense = Dense(64, activation='relu')\n",
    "        self.dens1 = Dense(32, activation='relu')\n",
    "        self.dens2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpo(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpo(x)\n",
    "        x = self.flatt(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.dens1(x)\n",
    "        return self.dens2(x)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    #The code in https://github.com/carlini/nn_robust_attacks assumes that the input image must ranges from -0.5 and 0.5 \n",
    "    train_data, test_data = train_data / 255.0 - 0.5, test_data / 255.0 - 0.5\n",
    "    train_data = train_data[..., tf.newaxis]\n",
    "    test_data = test_data[..., tf.newaxis]\n",
    "    train_data = tf.cast(train_data, tf.float32)\n",
    "    test_data = tf.cast(test_data, tf.float32)\n",
    "\n",
    "    train_labels = tf.one_hot(train_labels, 10)\n",
    "    test_labels = tf.one_hot(test_labels, 10)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(60000).batch(32)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(32)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def train(model_file='./mnist/trained_model', num_epochs=5, init=None):\n",
    "\n",
    "    train_ds, test_ds = load_data()\n",
    "\n",
    "    model = MNISTModel()\n",
    "\n",
    "    optimizer = optimizers.Adam()\n",
    "\n",
    "    train_loss = metrics.Mean(name='train_loss')\n",
    "    train_accuracy = metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = metrics.Mean(name='test_loss')\n",
    "    test_accuracy = metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    if init != None:\n",
    "        model.load_weights(model_file)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(images)\n",
    "            loss_value = loss_object(labels, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss(loss_value)\n",
    "        train_accuracy(labels, logits)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(images, labels):\n",
    "        logits = model(images)\n",
    "        loss_value = loss_object(labels, logits)\n",
    "        test_loss(loss_value)\n",
    "        test_accuracy(labels, logits)\n",
    "\n",
    "    def loss_object(labels, logits):\n",
    "        return tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_ds:\n",
    "            train_step(images, labels)\n",
    "\n",
    "        for test_images, test_labels in test_ds:\n",
    "            test_step(test_images, test_labels)\n",
    "\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch + 1, train_loss.result(), train_accuracy.result()\n",
    "                               * 100, test_loss.result(), test_accuracy.result() * 100))\n",
    "\n",
    "    model.save_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14034052193164825, Accuracy: 95.71833801269531, Test Loss: 0.0559983104467392, Test Accuracy: 98.31999969482422\n",
      "Epoch 2, Loss: 0.09270289540290833, Accuracy: 97.19833374023438, Test Loss: 0.049338601529598236, Test Accuracy: 98.44999694824219\n",
      "Epoch 3, Loss: 0.07264893501996994, Accuracy: 97.79999542236328, Test Loss: 0.04247733950614929, Test Accuracy: 98.66667175292969\n",
      "Epoch 4, Loss: 0.06063837558031082, Accuracy: 98.16583251953125, Test Loss: 0.03830697759985924, Test Accuracy: 98.80999755859375\n",
      "Epoch 5, Loss: 0.05280722305178642, Accuracy: 98.39266204833984, Test Loss: 0.036366648972034454, Test Accuracy: 98.88399505615234\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carlini and Wagner Attack\n",
    "\n",
    "ORIGINAL PAPER --> https://arxiv.org/abs/1608.04644\n",
    "\n",
    "The Carlini and Wagner method is currently one of the strongest family of adversarial attacks. The core insight transforms a general constrained optimization\n",
    "strategy similar to the L-BFGS attack into an empirically-chosen loss function within an unconstrained optimization formulation.\n",
    "\n",
    "They define the problem of finding an adversarial instance for an image $x$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{minimize }& D(x, x + \\delta)\\\\\n",
    "\\text{such that }& C(x + \\delta) = t \\\\\n",
    "& x + \\delta \\in [0,1]^n\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $x$ is fixed, $C(x)$ is the class label returned with an image $x$, and the goal is to find $\\delta$ that minimizes $D(x, x + \\delta)$. That is, we want to find some small change $\\delta$ that we can make to an image $x$ that will change its classification, but so that the result is still a valid image. Here $D$ is some distance metric; for us, it will be either $L_{0}$, $L_{2}$, or $L_{\\infty}$, which are widely-used distance metrics in the literature for generating adversarial examples, all of which are $L_{p}$ norms. The $L_{p}$ distance is written $\\Vert x − x^{\\prime} \\Vert$, where the p-norm $\\Vert \\cdot \\Vert_{p}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\Vert \\nu \\Vert_{p} = \\left(\\sum_{i=1}^{n} \\vert v_{i} \\vert ^{p} \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "\n",
    "The above formulation is difficult for existing algorithms to solve directly, as the constraint $C(x + \\delta) = t$ is highly non-linear. Therefore, they express it in a different form that is better suited for optimization. They introduces a function $f$ such that $C(x + \\delta) = t$ if and only if $f(x + \\delta) \\leq 0$. There are many choices of $f$ presented in the original article. The recommended choice of such function $f$ is:\n",
    "\n",
    "$$\n",
    "f(x) = \\left([\\textrm{max}_{i \\ne t}Z(x)_i] - Z(x)_t \\right)^+\n",
    "$$\n",
    "\n",
    "where $Z(x)_t$ is the output of the DNN before the softmax layer (logits) for the class $t$ and $(e)^{+}$ is short-hand for $max(e, 0)$.\n",
    "\n",
    "Therefore, they adjusted some of the above formula. Now, instead of formulating the problem as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{minimize }& D(x, x + \\delta)\\\\\n",
    "\\text{such that }& C(x + \\delta) = t \\\\\n",
    "& x + \\delta \\in [0,1]^n\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "they use the alternative formulation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{minimize }& D(x, x + \\delta) + c \\cdot f(x + \\delta)\\\\\n",
    "\\text{such that }& x + \\delta \\in [0,1]^n\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Notice that we added a constant. They have done this only so that the function respects the definition of optimization problem. This does not impact the final result, as it just scales the minimization function.\n",
    "\n",
    "## Box constraints\n",
    "\n",
    "To ensure the modification yields a valid image, we have a constraint on $\\delta$: we must have $0 \\leq x_{i} + \\delta_{i} \\leq 1$ for all $i$. In the\n",
    "optimization literature, this is known as a **box constraint.** This is done by the change of variable techinique, introducing the new variable $w_{i}$ such that:\n",
    "\n",
    "$$\n",
    "\\delta_i = \\frac{1}{2} \\left( \\textrm{tanh}(w_i)+1\\right)-x_i\n",
    "$$\n",
    "\n",
    "Since $−1 \\leq tanh(w_{i}) \\leq 1$, it follows that $0 \\leq x_{i} + \\delta_{i} \\leq 1$, so the solution will automatically be valid.\n",
    "\n",
    "\n",
    "They use the Adam optimizer (https://arxiv.org/abs/1412.6980) almost exclusively, as we have found it to be the most effective at quickly finding adversarial examples\n",
    "\n",
    "## L2 Attack\n",
    "\n",
    "In this tutorial, we will show $L_{2}$ attack, where $p = 2$, the objective function is minimized using the gradient descent.\n",
    "\n",
    "![](images/cw_l2.png)\n",
    "\n",
    "$\\kappa$ represents a parameter that reflects the minimum desired confidence margin for the adversarial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/carlini/nn_robust_attacks\n",
    "    \n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 1000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n",
    "TARGETED = True          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0           # how strong the adversarial example should be\n",
    "INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess\n",
    "\n",
    "\n",
    "class CarliniL2:\n",
    "    def __init__(self, model, batch_size=1, confidence=CONFIDENCE, targeted=TARGETED, learning_rate=LEARNING_RATE, binary_search_steps=BINARY_SEARCH_STEPS, max_iterations=MAX_ITERATIONS, abort_early=ABORT_EARLY, initial_const=INITIAL_CONST, boxmin=-0.5, boxmax=0.5):\n",
    "        # image_size, num_channels, num_labels = model.image_size, model.num_channels, model.num_labels\n",
    "        image_size, num_channels, num_labels = 28, 1, 10\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.BINARY_SEARCH_STEPS = binary_search_steps\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.CONFIDENCE = confidence\n",
    "        self.initial_const = initial_const\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.repeat = binary_search_steps >= 10\n",
    "\n",
    "        self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = False\n",
    "\n",
    "        self.shape = (batch_size, image_size, image_size, num_channels)\n",
    "        self.boxmul = (boxmax - boxmin) / 2.\n",
    "        self.boxplus = (boxmin + boxmax) / 2.\n",
    "        self.model = model\n",
    "\n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the L_2 attack on the given images for the given targets.\n",
    "\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        print('go up to', len(imgs))\n",
    "        for i in range(0, len(imgs), self.batch_size):\n",
    "            print('tick', i)\n",
    "            # print(\"imgs[i:i + self.batch_size]\", imgs[i:i + self.batch_size])\n",
    "            # print(\"targets\", targets)\n",
    "            r.extend(self.attack_batch(\n",
    "                imgs[i:i + self.batch_size], targets))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_batch(self, imgs, labs):\n",
    "        \"\"\"\n",
    "        Run the attack on a batch of images and labels.\n",
    "        \"\"\"\n",
    "        # print(\"imgs, labs in attack_batch\", imgs, labs) #shape=(1, 28, 28, 1), dtype=float32) [array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])]\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        def compare(x, y):\n",
    "\n",
    "            if not isinstance(x, (float, int, np.int64)):\n",
    "                x = x.numpy()\n",
    "                x = np.copy(x)\n",
    "                if self.TARGETED:\n",
    "                    x[y] -= self.CONFIDENCE\n",
    "                else:\n",
    "                    x[y] += self.CONFIDENCE\n",
    "                x = np.argmax(x)\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "\n",
    "        # @tf.function\n",
    "        def train_step(modifier, timg, tlab, const):\n",
    "            with tf.GradientTape() as tape:\n",
    "                newimg = tf.tanh(modifier + timg) * self.boxmul + self.boxplus\n",
    "                # newimg = np.random.rand(1, 28, 28, 1)\n",
    "                output = self.model(newimg)\n",
    "                output = tf.cast(output, dtype=tf.float32)\n",
    "                l2dist = tf.reduce_sum(tf.square(newimg - (tf.tanh(timg) * self.boxmul + self.boxplus)), [1, 2, 3])\n",
    "                real = tf.math.reduce_sum((tlab) * output, 1)\n",
    "                other = tf.math.reduce_max(\n",
    "                    (1 - tlab) * output - (tlab * 10000), 1)\n",
    "                if self.TARGETED:\n",
    "                    # if targetted, optimize for making the other class most likely\n",
    "                    loss1 = tf.maximum(0.0, other - real + self.CONFIDENCE)\n",
    "                else:\n",
    "                    # if untargeted, optimize for making this class least likely.\n",
    "                    loss1 = tf.maximum(0.0, real - other + self.CONFIDENCE)\n",
    "\n",
    "                loss2 = tf.reduce_sum(l2dist)\n",
    "                loss1 = tf.reduce_sum(const * loss1)\n",
    "\n",
    "                loss = loss1 + loss2\n",
    "            optimizer = optimizers.Adam(self.LEARNING_RATE)\n",
    "            loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "            grads = tape.gradient(loss, [modifier])\n",
    "            optimizer.apply_gradients(zip(grads, [modifier]))\n",
    "            loss_metric.update_state(loss)\n",
    "            return loss, l2dist, output, newimg, loss1, loss2\n",
    "\n",
    "        # convert to tanh-space\n",
    "        imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n",
    "        # print(np.shape(imgs))\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        CONST = np.ones(batch_size) * self.initial_const\n",
    "        upper_bound = np.ones(batch_size) * 1e10\n",
    "\n",
    "        # the best l2, score, and image attack\n",
    "        o_bestl2 = [1e10] * batch_size\n",
    "        o_bestscore = [-1] * batch_size\n",
    "        o_bestattack = [np.zeros(imgs[0].shape)] * batch_size\n",
    "        print(np.shape(o_bestattack), \"np.shape(o_bestattack)\")  # (1, 28, 28, 1)\n",
    "\n",
    "        for outer_step in range(self.BINARY_SEARCH_STEPS):\n",
    "            batch = tf.Variable(imgs[:batch_size], dtype=tf.float32)\n",
    "            batchlab = tf.Variable(labs[:batch_size], dtype=tf.float32)\n",
    "            # print(\"*******batchlab***********\", batchlab)  # shape=(1, 10)\n",
    "            bestl2 = [1e10] * batch_size\n",
    "            bestscore = [-1] * batch_size\n",
    "            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS - 1:\n",
    "                CONST = upper_bound\n",
    "\n",
    "            modifier = tf.Variable(np.zeros((1, 28, 28, 1), dtype=np.float32))\n",
    "            const = tf.Variable(CONST, dtype=tf.float32)\n",
    "            prev = np.inf\n",
    "            for iteration in range(self.MAX_ITERATIONS):\n",
    "                # perform the attack\n",
    "\n",
    "                l, l2s, scores, nimg, loss1, loss2 = train_step(\n",
    "                    modifier, batch, batchlab, const)\n",
    "\n",
    "                if np.all(scores >= -.0001) and np.all(scores <= 1.0001):\n",
    "                    if np.allclose(np.sum(scores, axis=1), 1.0, atol=1e-3):\n",
    "                        if not self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK:\n",
    "                            raise Exception(\"The output of model.predict should return the pre-softmax layer. It looks like you are returning the probability vector (post-softmax). If you are sure you want to do that, set attack.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = True\")\n",
    "\n",
    "                if iteration % (self.MAX_ITERATIONS // 10) == 0:\n",
    "                    print(iteration, l, loss1, loss2)\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if self.ABORT_EARLY and iteration % (self.MAX_ITERATIONS // 10) == 0:\n",
    "                    if l > prev * .9999:\n",
    "                        break\n",
    "                    prev = l\n",
    "                # adjust the best result found so far\n",
    "                for e, (l2, sc, ii) in enumerate(zip(l2s, scores, nimg)):\n",
    "                    # print(\"batchlab\", np.argmax(batchlab[e]))\n",
    "                    # print(\"(sc, np.argmax(batchlab))\", sc, np.argmax(sc))\n",
    "                    # print(\"l2 and bestl2[e]\", l2, bestl2[e])\n",
    "                    # print(\"compare(sc, tf.argmax(batchlab))\",\n",
    "                    #       compare(sc, tf.argmax(batchlab[e])))\n",
    "                    if l2 < bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = np.argmax(sc)\n",
    "                    if l2 < o_bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = np.argmax(sc)\n",
    "                        o_bestattack[e] = ii\n",
    "\n",
    "                # adjust the constant as needed\n",
    "            for e in range(batch_size):\n",
    "                print(\"bestscore[e]\", bestscore[e])\n",
    "                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    #          or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "        o_bestl2 = np.array(o_bestl2)\n",
    "        return o_bestattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "go up to 1\n",
      "tick 0\n",
      "(1, 28, 28, 1) np.shape(o_bestattack)\n",
      "0 tf.Tensor(0.01717016, shape=(), dtype=float32) tf.Tensor(0.01717016, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(0.017286345, shape=(), dtype=float32) tf.Tensor(0.017189069, shape=(), dtype=float32) tf.Tensor(9.727733e-05, shape=(), dtype=float32)\n",
      "bestscore[e] -1\n",
      "0 tf.Tensor(0.17170158, shape=(), dtype=float32) tf.Tensor(0.17170158, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(0.17087348, shape=(), dtype=float32) tf.Tensor(0.1697219, shape=(), dtype=float32) tf.Tensor(0.0011515766, shape=(), dtype=float32)\n",
      "200 tf.Tensor(0.17088287, shape=(), dtype=float32) tf.Tensor(0.16972218, shape=(), dtype=float32) tf.Tensor(0.0011606757, shape=(), dtype=float32)\n",
      "bestscore[e] -1\n",
      "0 tf.Tensor(1.7170159, shape=(), dtype=float32) tf.Tensor(1.7170159, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(1.5953355, shape=(), dtype=float32) tf.Tensor(1.475372, shape=(), dtype=float32) tf.Tensor(0.11996353, shape=(), dtype=float32)\n",
      "200 tf.Tensor(1.5899931, shape=(), dtype=float32) tf.Tensor(1.4672266, shape=(), dtype=float32) tf.Tensor(0.122766465, shape=(), dtype=float32)\n",
      "300 tf.Tensor(1.5898335, shape=(), dtype=float32) tf.Tensor(1.467373, shape=(), dtype=float32) tf.Tensor(0.122460485, shape=(), dtype=float32)\n",
      "400 tf.Tensor(1.5898031, shape=(), dtype=float32) tf.Tensor(1.4675149, shape=(), dtype=float32) tf.Tensor(0.1222882, shape=(), dtype=float32)\n",
      "bestscore[e] -1\n",
      "0 tf.Tensor(17.170158, shape=(), dtype=float32) tf.Tensor(17.170158, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(10.677559, shape=(), dtype=float32) tf.Tensor(8.040546, shape=(), dtype=float32) tf.Tensor(2.637012, shape=(), dtype=float32)\n",
      "200 tf.Tensor(10.184341, shape=(), dtype=float32) tf.Tensor(7.2999306, shape=(), dtype=float32) tf.Tensor(2.8844109, shape=(), dtype=float32)\n",
      "300 tf.Tensor(10.148394, shape=(), dtype=float32) tf.Tensor(7.241283, shape=(), dtype=float32) tf.Tensor(2.9071112, shape=(), dtype=float32)\n",
      "400 tf.Tensor(10.146299, shape=(), dtype=float32) tf.Tensor(7.2394724, shape=(), dtype=float32) tf.Tensor(2.9068274, shape=(), dtype=float32)\n",
      "500 tf.Tensor(10.144208, shape=(), dtype=float32) tf.Tensor(7.237672, shape=(), dtype=float32) tf.Tensor(2.906536, shape=(), dtype=float32)\n",
      "600 tf.Tensor(10.140075, shape=(), dtype=float32) tf.Tensor(7.233947, shape=(), dtype=float32) tf.Tensor(2.906128, shape=(), dtype=float32)\n",
      "700 tf.Tensor(10.031804, shape=(), dtype=float32) tf.Tensor(7.1224384, shape=(), dtype=float32) tf.Tensor(2.9093657, shape=(), dtype=float32)\n",
      "800 tf.Tensor(9.984273, shape=(), dtype=float32) tf.Tensor(7.0437427, shape=(), dtype=float32) tf.Tensor(2.94053, shape=(), dtype=float32)\n",
      "900 tf.Tensor(9.816889, shape=(), dtype=float32) tf.Tensor(6.795537, shape=(), dtype=float32) tf.Tensor(3.0213513, shape=(), dtype=float32)\n",
      "bestscore[e] -1\n",
      "0 tf.Tensor(171.70158, shape=(), dtype=float32) tf.Tensor(171.70158, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(71.15135, shape=(), dtype=float32) tf.Tensor(65.56836, shape=(), dtype=float32) tf.Tensor(5.5829906, shape=(), dtype=float32)\n",
      "200 tf.Tensor(38.249657, shape=(), dtype=float32) tf.Tensor(29.755375, shape=(), dtype=float32) tf.Tensor(8.494283, shape=(), dtype=float32)\n",
      "300 tf.Tensor(23.98714, shape=(), dtype=float32) tf.Tensor(12.732145, shape=(), dtype=float32) tf.Tensor(11.254995, shape=(), dtype=float32)\n",
      "400 tf.Tensor(12.929578, shape=(), dtype=float32) tf.Tensor(0.42760372, shape=(), dtype=float32) tf.Tensor(12.501974, shape=(), dtype=float32)\n",
      "500 tf.Tensor(11.6495695, shape=(), dtype=float32) tf.Tensor(0.19038677, shape=(), dtype=float32) tf.Tensor(11.459183, shape=(), dtype=float32)\n",
      "600 tf.Tensor(11.580668, shape=(), dtype=float32) tf.Tensor(0.57700634, shape=(), dtype=float32) tf.Tensor(11.003662, shape=(), dtype=float32)\n",
      "700 tf.Tensor(11.094523, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(11.094523, shape=(), dtype=float32)\n",
      "800 tf.Tensor(11.037115, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(11.037115, shape=(), dtype=float32)\n",
      "900 tf.Tensor(11.029921, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(11.029921, shape=(), dtype=float32)\n",
      "bestscore[e] 9\n",
      "0 tf.Tensor(94.43587, shape=(), dtype=float32) tf.Tensor(94.43587, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(42.183945, shape=(), dtype=float32) tf.Tensor(37.349735, shape=(), dtype=float32) tf.Tensor(4.834211, shape=(), dtype=float32)\n",
      "200 tf.Tensor(30.972809, shape=(), dtype=float32) tf.Tensor(24.398773, shape=(), dtype=float32) tf.Tensor(6.5740366, shape=(), dtype=float32)\n",
      "300 tf.Tensor(19.504345, shape=(), dtype=float32) tf.Tensor(11.303485, shape=(), dtype=float32) tf.Tensor(8.200861, shape=(), dtype=float32)\n",
      "400 tf.Tensor(13.665012, shape=(), dtype=float32) tf.Tensor(3.6133351, shape=(), dtype=float32) tf.Tensor(10.051678, shape=(), dtype=float32)\n",
      "500 tf.Tensor(10.1219015, shape=(), dtype=float32) tf.Tensor(0.2273457, shape=(), dtype=float32) tf.Tensor(9.894556, shape=(), dtype=float32)\n",
      "600 tf.Tensor(9.395503, shape=(), dtype=float32) tf.Tensor(0.0878284, shape=(), dtype=float32) tf.Tensor(9.307674, shape=(), dtype=float32)\n",
      "700 tf.Tensor(9.325161, shape=(), dtype=float32) tf.Tensor(0.22549415, shape=(), dtype=float32) tf.Tensor(9.099667, shape=(), dtype=float32)\n",
      "800 tf.Tensor(9.079346, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(9.079346, shape=(), dtype=float32)\n",
      "900 tf.Tensor(9.615762, shape=(), dtype=float32) tf.Tensor(0.57023156, shape=(), dtype=float32) tf.Tensor(9.04553, shape=(), dtype=float32)\n",
      "bestscore[e] 9\n",
      "0 tf.Tensor(55.803017, shape=(), dtype=float32) tf.Tensor(55.803017, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(26.92767, shape=(), dtype=float32) tf.Tensor(22.517813, shape=(), dtype=float32) tf.Tensor(4.409856, shape=(), dtype=float32)\n",
      "200 tf.Tensor(24.384695, shape=(), dtype=float32) tf.Tensor(19.236666, shape=(), dtype=float32) tf.Tensor(5.148029, shape=(), dtype=float32)\n",
      "300 tf.Tensor(16.131811, shape=(), dtype=float32) tf.Tensor(9.517816, shape=(), dtype=float32) tf.Tensor(6.613995, shape=(), dtype=float32)\n",
      "400 tf.Tensor(13.569904, shape=(), dtype=float32) tf.Tensor(6.465674, shape=(), dtype=float32) tf.Tensor(7.104231, shape=(), dtype=float32)\n",
      "500 tf.Tensor(11.413587, shape=(), dtype=float32) tf.Tensor(3.3697214, shape=(), dtype=float32) tf.Tensor(8.043865, shape=(), dtype=float32)\n",
      "600 tf.Tensor(9.407845, shape=(), dtype=float32) tf.Tensor(0.55533004, shape=(), dtype=float32) tf.Tensor(8.852514, shape=(), dtype=float32)\n",
      "700 tf.Tensor(8.635151, shape=(), dtype=float32) tf.Tensor(0.19504398, shape=(), dtype=float32) tf.Tensor(8.440107, shape=(), dtype=float32)\n",
      "800 tf.Tensor(8.201249, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(8.201249, shape=(), dtype=float32)\n",
      "900 tf.Tensor(8.2050705, shape=(), dtype=float32) tf.Tensor(0.37480152, shape=(), dtype=float32) tf.Tensor(7.830269, shape=(), dtype=float32)\n",
      "bestscore[e] 9\n",
      "0 tf.Tensor(36.486588, shape=(), dtype=float32) tf.Tensor(36.486588, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(18.943157, shape=(), dtype=float32) tf.Tensor(14.906223, shape=(), dtype=float32) tf.Tensor(4.0369344, shape=(), dtype=float32)\n",
      "200 tf.Tensor(17.691488, shape=(), dtype=float32) tf.Tensor(13.329225, shape=(), dtype=float32) tf.Tensor(4.3622637, shape=(), dtype=float32)\n",
      "300 tf.Tensor(17.18356, shape=(), dtype=float32) tf.Tensor(12.669945, shape=(), dtype=float32) tf.Tensor(4.5136147, shape=(), dtype=float32)\n",
      "400 tf.Tensor(17.171404, shape=(), dtype=float32) tf.Tensor(12.65021, shape=(), dtype=float32) tf.Tensor(4.5211935, shape=(), dtype=float32)\n",
      "500 tf.Tensor(17.166237, shape=(), dtype=float32) tf.Tensor(12.644698, shape=(), dtype=float32) tf.Tensor(4.5215383, shape=(), dtype=float32)\n",
      "600 tf.Tensor(17.026926, shape=(), dtype=float32) tf.Tensor(12.518131, shape=(), dtype=float32) tf.Tensor(4.508795, shape=(), dtype=float32)\n",
      "700 tf.Tensor(16.445274, shape=(), dtype=float32) tf.Tensor(11.744316, shape=(), dtype=float32) tf.Tensor(4.700959, shape=(), dtype=float32)\n",
      "800 tf.Tensor(12.18412, shape=(), dtype=float32) tf.Tensor(6.7128158, shape=(), dtype=float32) tf.Tensor(5.471304, shape=(), dtype=float32)\n",
      "900 tf.Tensor(9.977956, shape=(), dtype=float32) tf.Tensor(4.417524, shape=(), dtype=float32) tf.Tensor(5.560432, shape=(), dtype=float32)\n",
      "bestscore[e] -1\n",
      "0 tf.Tensor(46.144802, shape=(), dtype=float32) tf.Tensor(46.144802, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "100 tf.Tensor(23.092821, shape=(), dtype=float32) tf.Tensor(18.941618, shape=(), dtype=float32) tf.Tensor(4.1512036, shape=(), dtype=float32)\n",
      "200 tf.Tensor(21.020905, shape=(), dtype=float32) tf.Tensor(16.223211, shape=(), dtype=float32) tf.Tensor(4.797693, shape=(), dtype=float32)\n",
      "300 tf.Tensor(14.846514, shape=(), dtype=float32) tf.Tensor(9.036418, shape=(), dtype=float32) tf.Tensor(5.810096, shape=(), dtype=float32)\n",
      "400 tf.Tensor(12.146151, shape=(), dtype=float32) tf.Tensor(5.830498, shape=(), dtype=float32) tf.Tensor(6.315653, shape=(), dtype=float32)\n",
      "500 tf.Tensor(10.109017, shape=(), dtype=float32) tf.Tensor(2.8929503, shape=(), dtype=float32) tf.Tensor(7.2160673, shape=(), dtype=float32)\n",
      "600 tf.Tensor(9.143982, shape=(), dtype=float32) tf.Tensor(1.5475527, shape=(), dtype=float32) tf.Tensor(7.5964293, shape=(), dtype=float32)\n",
      "700 tf.Tensor(8.328934, shape=(), dtype=float32) tf.Tensor(0.28814653, shape=(), dtype=float32) tf.Tensor(8.040787, shape=(), dtype=float32)\n",
      "800 tf.Tensor(8.059577, shape=(), dtype=float32) tf.Tensor(0.26883495, shape=(), dtype=float32) tf.Tensor(7.790742, shape=(), dtype=float32)\n",
      "900 tf.Tensor(7.761038, shape=(), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(7.761038, shape=(), dtype=float32)\n",
      "bestscore[e] 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distortion: 2.7516462427032495\n"
     ]
    }
   ],
   "source": [
    "model = MNISTModel()\n",
    "model.load_weights(\"./mnist/trained_model\")\n",
    "attack = CarliniL2(model)\n",
    "\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_data, test_data = train_data / 255.0 - 0.5, test_data / 255.0 - 0.5\n",
    "train_data = train_data[..., tf.newaxis]\n",
    "test_data = test_data[..., tf.newaxis]\n",
    "train_data = tf.cast(train_data, tf.float32)\n",
    "test_data = tf.cast(test_data, tf.float32)\n",
    "\n",
    "# inputs = train_data[25]  # tf.Tensor[(28, 28, 1)]\n",
    "inputs = train_data[25:26]  # tf.Tensor[(1, 28, 28, 1)]\n",
    "\n",
    "target_index = 9\n",
    "targets = np.eye(10)[target_index]\n",
    "\n",
    "print(targets)\n",
    "list_targets = []\n",
    "list_targets.append(targets)\n",
    "\n",
    "adv = attack.attack(inputs, list_targets)\n",
    "\n",
    "for i in range(len(adv)):\n",
    "    logits_image = model.predict(inputs[i:i+1])\n",
    "    class_image = logits_image.argmax()\n",
    "    confidence_image = tf.nn.softmax(logits_image).numpy()[0][class_image]\n",
    "\n",
    "    logits_image_adv = model.predict(adv[i:i+1])\n",
    "    class_image_adv = logits_image_adv.argmax()\n",
    "    confidence_image_adv = tf.nn.softmax(logits_image_adv).numpy()[0][class_image]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,18))\n",
    "\n",
    "    confidence = str(round(confidence_image,2))\n",
    "    pixels = inputs[i].numpy().reshape((28, 28))\n",
    "    axes[0].imshow(pixels, cmap='gray')\n",
    "    axes[0].set_axis_off()\n",
    "    title = 'Original\\n Predicted: {0} Confidence: {1}'\n",
    "    axes[0].set_title(title.format(class_image, confidence))\n",
    "\n",
    "\n",
    "    confidence = str(round(confidence_image_adv,2))\n",
    "    pixels = adv[i].reshape((28, 28))\n",
    "    axes[1].imshow(pixels, cmap='gray')\n",
    "    axes[1].set_axis_off()\n",
    "    title = 'Adversarial\\n Predicted: {0} Confidence: {1}'\n",
    "    axes[1].set_title(title.format(class_image_adv, confidence))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"Total distortion:\", np.sum((adv[i] - inputs[i])**2)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
